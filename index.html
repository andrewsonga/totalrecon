
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Total-Recon</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://andrewsonga.github.io/totalrecon"/>
    <meta property="og:title" content="Total-Recon"/>
    <meta property="og:description" content="Project page for Total-Recon: Holistic Multibody Deformable Scene Reconstruction from RGBD Video." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Total-Recon" />
    <meta name="twitter:description" content="Project page for Total-Recon: Holistic Multibody Deformable Scene Reconstruction from RGBD Video." />
    <meta name="twitter:image" content="my_figures/totalnerf_method.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Total-Recon</b>: Holistic Multibody Deformable Scene Reconstruction from RGBD Video</br> 
                <!--small> -->
                <!--/small> -->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://andrewsonga.github.io">
                          Chonghyuk Song
                        </a>
                        </br>CMU
                    </li>
                    <li>
                        <a href="https://gengshan-y.github.io">
                          Gengshan Yang
                        </a>
                        </br>CMU
                    </li>
                    <li>
                        <a href="https://dunbar12138.github.io">
                          Kangle Deng  
                        </a>
                        </br>CMU
                    </li><br>
                    <li>
                        <a href="https://www.cs.cmu.edu/~junyanz/">
                          Jun-Yan Zhu
                        </a>
                        </br>CMU
                    </li>
                    <li>
                        <a href="https://www.cs.cmu.edu/~deva/">
                          Deva Ramanan
                        </a>
                        </br>CMU
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="">
                            <image src="my_figures/totalnerf_paper_image_border.png" height="60px">
                                <h4><strong>Paper<br>[coming soon]</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="my_figures/youtube_icon.png" height="60px">
                                <h4><strong>Video<br>[coming soon]</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="my_figures/github.png" height="60px">
                                <h4><strong>Code & Data<br>[coming soon]</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <!-- <image src="img/rays.png" class="img-responsive" alt="overview"><br> -->
                    <video  width=100% height=100% muted autoplay loop>
                    <source src="my_figures/nvs-all-humandog.mp4" type="video/mp4">
                    </video>
                    <br><br>
                <p class="text-justify">
                    Prior approaches to monocular-view neural reconstruction have largely been restricted to rigidly moving scenes, or short videos containing limited deformation. However, such works do not scale to minute-long videos featuring highly dynamic and deformable objects (such as humans and pets), the type of casual videos captured in our day-to-day lives and the main focus of user content. 
                    We thus present a method that recovers a 3D reconstruction of a dynamic, deformable scene from a casual RGBD video captured by a moving iPad Pro, a commodity depth sensor currently in the hands of millions of people. Our key insight to enabling holistic, multi-body reconstruction of deformable scenes merges three schools of thought: (1) an animatable 3D neural model capable of reconstructing highly deformable objects, (2) a compositional 3D scene representation that decomposes the scene into multiple deforming objects and a rigid background, and most importantly, (3) depth supervision, which resolves ambiguities inherent to multi-body monocular reconstruction, allowing for truly metric reconstruction. 
                    To evaluate our approach, we introduce a new dataset of challenging RGBD videos containing various deformable subjects and demonstrate for the first time 3D reconstructions of people interacting with their pets. To enable quantitative evaluation of novel-view synthesis, we build a stereo RGBD-sensor capture rig for ground-truthing. Furthermore, our method enables novel applications such as third-person- and egocentric-view synthesis and 3D AR filters. To the best of our knowledge, we are also the first to showcase such applications of monocular-view, non-rigid scene reconstruction.
                </p>
            </div>
        </div>


        <!--
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/EpH175PY1A0" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>
        -->

        <!--
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Integrated Positional Encoding
                </h3>
                <p class="text-justify">
                    Typical positional encoding (as used in Transformer networks and Neural Radiance Fields) maps a single point in space to a feature vector, where each element is generated by a sinusoid with an exponentially increasing frequency:
                </p>
                <p style="text-align:center;">
                    <image src="img/pe_seq_eqn_pad.png" height="50px" class="img-responsive">
                </p>
                <video id="v0" width="100%" autoplay loop muted>
                  <source src="img/pe_anim_horiz.mp4" type="video/mp4" />
                </video>
                <p class="text-justify">
                    Here, we show how these feature vectors change as a function of a point moving in 1D space.
                    <br><br>
                    Our <em>integrated positional encoding</em> considers Gaussian <em>regions</em> of space, rather than infinitesimal points. This provides a natural way to input a "region" of space as query to a coordinate-based neural network, allowing the network to reason about sampling and aliasing. The expected value of each positional encoding component has a simple closed form:
                </p>
                <p style="text-align:center;">
                    <image src="img/ipe_eqn_under_pad.png" height="30px" class="img-responsive">
                </p>
                <video id="v0" width="100%" autoplay loop muted>
                  <source src="img/ipe_anim_horiz.mp4" type="video/mp4" />
                </video>
                <p class="text-justify">
                    We can see that when considering a wider region, the higher frequency features automatically shrink toward zero, providing the network with lower-frequency inputs. As the region narrows, these features converge to the original positional encoding.
                </p>
            </div>
        </div>
        -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Total-Recon
                </h3>
                <p class="text-justify">
                    Total-Recon represents the entire scene as a composition of neural fields, one for each deformable foreground object (modeled by <a href="https://banmo-www.github.io">BANMo</a>) and the rigid background (modeled by a <a href="https://www.matthewtancik.com/nerf">NeRF</a>). 
                </p>
                <p style="text-align:center;">
                    <image src="my_figures/totalnerf_method.png" class="img-responsive" alt="scales">
                </p>
                <p class="text-justify">
                    The composite field is computed by first transforming the individual <strong>1) object fields</strong> into the camera space via per-object rigid transformations ("Pose") and per-object deformation fields ("Deform") for foreground objects, then combining the <strong>2) posed object fields</strong>. 
                    Lastly, volume rendering is performed on the <strong>3) composite field</strong> to render <strong>4) color, depth, optical flow, and object masks</strong>, each of which defines a reconstruction loss that derives supervision from a monocular RGBD video captured by a moving iPad Pro.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>

                <h4>
                    1) 3D Reconstruction and Downstream Applications <a href="applications.html">[Click for more sequences]</a>
                </h4>
                <table align=center width="100%">
                    <tr>
                      <td colspan=1 width="12.5%"></td>
                      <td colspan=2 width="25%" style="font-size: 100%"><center><strong>Input RGBD Video</strong></center></td>
                      <td colspan=1 width="12.5%" style="font-size: 100%"><center><strong>3D<br>Reconstruction</strong></center></td>
                      <td colspan=1 width="12.5%" style="font-size: 100%"><center><strong><span style="color:blue">3rd-Person/ </br> Pet View</span></strong></center></td>
                      <td colspan=1 width="12.5%" style="font-size: 100%"><center><strong><span style="color:#dbc300">Egocentric View</span></strong></center></td>
                      <td colspan=1 width="12.5%" style="font-size: 100%"><center><strong>Bird's Eye View</strong></center></td>
                      <td colspan=1 width="12.5%" style="font-size: 100%"><center><strong><span style="color:cyan">3D AR Filter</span></strong></center></td>
                    </tr>
                    <tr>
                      <td style="font-size: 100%"><center>Human 2 &<br /> Cat 1</center></td>
                      <td colspan=7><video style="vertical-align: bottom;" autoplay="autoplay" controls="controls" loop="loop" muted="" playsinline="" src="my_figures/applications/humancat/nvs-all.mp4" width="100%"></video></td>
                    </tr>
                    <tr>
                      <td style="font-size: 100%"><center>Dog 1</center></td>
                      <td colspan=7><video style="vertical-align: bottom;" autoplay="autoplay" controls="controls" loop="loop" muted="" playsinline="" src="my_figures/applications/dog1/nvs-all.mp4" width="100%"></video></td>
                    </tr>
                </table>
                <br>
                <p class="text-justify">
                    We train Total-Recon to holistically reconstruct the scene for a variety of RGBD videos. The 3rd-person / pet and egocentric views are captured from the blue and yellow camera meshes shown in the 3D reconstruction, respectively. To showcase our method's AR capabilities, we attach a sky-blue unicorn horn to the forehead of the foreground object, which is automatically propagated across all frames.
                </p>    
                <br>
                <!--
                <br>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="my_figures/applications/humancat/nvs-all.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="my_figures/applications/cat2/nvs-all.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
                    <source src="my_figures/applications/dog1/nvs-all.mp4" type="video/mp4" />
                </video>
                <br><br>
                -->

                <h4>
                    2) Novel View Synthesis: Comparisons to Baselines <a href="nvs.html">[Click for more sequences]</a>
                </h4>  
                <!-- <span style="font-family:Times New Roman; font-size: 120%">-->
                <table align=center width="100%">
                    <tr>
                      <td width="16.6%"></td>
                      <td width="16.6%"><center><span style="font-size: 100%">Novel View <br /> (GT)</span></center></td>
                      <td width = "16.6%"><center><span style="font-size: 100%"><strong>Ours <br /> (w/ depth)</strong></span></center></td>
                      <td width = "16.6%"><center><span style="font-size: 100%">D<sup>2</sup>NeRF <br /> (w/ depth)</span></center></td>
                      <td width = "16.6%"><center><span style="font-size: 100%">D<sup>2</sup>NeRF <br /> (w/o depth)</span></center></td>
                      <td width = "16.6%"><center><span style="font-size: 100%">HyperNeRF <br /> (w/o depth)</span></center></td>
                    </tr>
                    <tr>
                        <td  width = "16.6%" rowspan="2" style="font-size: 100%"><center>Cat 2</center></td>
                        <td  width = "83.3%" colspan="5"><video autoplay="autoplay" controls="controls" loop="loop" muted="" playsinline="" src="my_figures/nvs/cat2/nvs-rgb-all.mp4" width="100%"></video></td>
                    </tr>
                    <tr>
                        <td  width = "83.3%" colspan="5"><video autoplay="autoplay" controls="controls" loop="loop" muted="" playsinline="" src="my_figures/nvs/cat2/nvs-dph-all.mp4" width="100%"></video></td>
                    </tr>
                    <tr>
                        <td  width = "16.6%" rowspan="2" style="font-size: 100%"><center>Human 1</center></td>
                        <td  width = "83.3%" colspan="5"><video autoplay="autoplay" controls="controls" loop="loop" muted="" playsinline="" src="my_figures/nvs/human1/nvs-rgb-all.mp4" width="100%"></video></td>
                    </tr>
                    <tr>
                        <td  width = "83.3%" colspan="5"><video autoplay="autoplay" controls="controls" loop="loop" muted="" playsinline="" src="my_figures/nvs/human1/nvs-dph-all.mp4" width="100%"></video></td>
                    </tr>
                </table>
                <br>
                <p class="text-justify">
                    We compare Total-Recon (ours) to <a href="https://hypernerf.github.io">HyperNeRF</a>, <a href="https://d2nerf.github.io">D<sup>2</sup>NeRF</a>, and a depth-supervised version of D<sup>2</sup>NeRF on novel-view synthesis for RGBD sequences captured with a stereo validation rig we have built. While the baseline methods are only able to reconstruct the background at best, our method is able to reconstruct <em>both</em> the background and the moving foreground(s), demonstrating holistic scene reconstruction.
                </p>
                <br>

                <h4>
                    3) Ablation Study on Depth Supervision <a href="ablation_depth.html">[Click for more sequences]</a>
                </h4>
                <table align=center width='100%'>
                    <tr>
                    <td width = "16.6%"></td>
                    <td width = "16.6%" style="font-size: 120%"><center><strong> Novel View<br /> (GT) </strong></center></td>
                    <td colspan="2" width = "33.3%" style="font-size: 120%"><center><span style="color:#3babb6"><strong> Depth-supervised<br /> (Rendered)</strong></span></center></td>
                    <td colspan="2" width = "33.3%" style="font-size: 120%"><center><span style="color:#d66501"><strong> No Depth Supervision<br /> (Rendered)</strong></h5></span></center></td>
                    </tr>
                    <tr>
                    <td style="font-size: 100%"><center>Human 1 &<br /> Dog 1</center></td>
                    <td colspan="5" width = "83.3%"><video autoplay="autoplay" controls="controls" loop="loop" muted="" playsinline="" src="my_figures/ablation_depth/humandog/nvs-all.mp4" width="100%"></video></td>
                    </tr>
                    <tr>
                    <td style="font-size: 100%"><center>Human 2 &<br /> Cat 1</center></td>
                    <td colspan="5" width = "83.3%"><video autoplay="autoplay" controls="controls" loop="loop" muted="" playsinline="" src="my_figures/ablation_depth/humancat/nvs-all.mp4" width="100%"></video></td>
                    </tr>
                </table>
                <br>
                <p class="text-justify">
                    While removing depth supervision doesn't significantly hamper the rendered RGB, it induces several failure modes as shown in the 3D reconstructions: 1) <strong>Floating objects</strong> in the <em>Human & Dog</em> sequence 2) <strong>Objects that sink into the background</strong> in the <em>Human & Cat</em> sequence 3) <strong>Lower reconstructon quality</strong>.
                </p> 
            </div>
        </div>

        <!--
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Related links
                </h3>
                <p class="text-justify">
                    <a href="https://en.wikipedia.org/wiki/Spatial_anti-aliasing">Wikipedia</a> provides an excellent introduction to spatial anti-aliasing techniques.
                </p>
                <p class="text-justify">
                    Mipmaps were introduced by Lance Williams in his paper "Pyramidal Parametrics" (<a href="https://software.intel.com/sites/default/files/m/7/2/c/p1-williams.pdf">Williams (1983)</a>).
                </p>
                <p class="text-justify">
                    <a href="https://dl.acm.org/doi/abs/10.1145/964965.808589">Amanatides (1984)</a> first proposed the idea of replacing rays with cones in computer graphics rendering. 
                </p>
                <p class="text-justify">
                    The closely related concept of <em>ray differentials</em> (<a href="https://graphics.stanford.edu/papers/trd/">Igehy (1999)</a>) is used in most modern renderers to antialias textures and other material buffers during ray tracing.
                </p>
                <p class="text-justify">
                    Cone tracing has been used along with prefiltered voxel-based representations of scene geometry for speeding up indirect illumination calculations in <a href="https://research.nvidia.com/sites/default/files/publications/GIVoxels-pg2011-authors.pdf">Crassin et al. (2011)</a>.
                </p>
                <p class="text-justify">
                    Mip-NeRF was implemented on top of the <a href="https://github.com/google-research/google-research/tree/master/jaxnerf">JAXNeRF</a> codebase.
                </p>
            </div>
        </div>
        -->
        
        <!--    
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{
}</textarea>
                </div>
            </div>
        </div>
        -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We thank Nathaniel Chodosh, Jeff Tan, George Cazenavette, and Jason Zhang for proofreading our paper. We thank Sheng-Yu Wang and Daohan (Fred) Lu as well for providing valuable feedback. This work is supported in part by the Sony Corporation and the CMU Argo AI Center for Autonomous Vehicle Research.
                <br>
                <br>
                The website template was borrowed from <a href="https://github.com/jonbarron/website">Jon Barron</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
